{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and export the raw patent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-222bd40ef57c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_patent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Network data/pub_patent_edges.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_patent = pd.read_csv('../Network data/pub_patent_edges.csv', dtype = 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of patentIDs (got to include the double quotes); delete duplicates\n",
    "data = json.dumps(list(set(df_patent['patentID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(df_patent['patentID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Response.json of <Response [200]>>\n",
      "--- 0.9220991134643555 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## for testing the scraper on one patent\n",
    "# I am not sure about the legal status of these patents (are they still mantained?)\n",
    "\n",
    "data_dict = []\n",
    "start_time = time.time()\n",
    "\n",
    "url = 'https://api.lens.org/patent/search'\n",
    "data_patent = '''{\n",
    "              \"query\": {\n",
    "                  \"terms\":  {\n",
    "                      \"lens_id\": [\"013-408-491-874-758\"]\n",
    "                              }\n",
    "                          },\n",
    "              \"include\": [\"doc_key\",\"biblio.parties\", \"biblio.invention_title\", \"biblio.references_cited\", \"biblio.cited_by\", \"jurisdiction\", \"date_published\", \"publication_type\"]\n",
    "            \n",
    "            }''' \n",
    "headers = {'Authorization': 'IjpxxGZZQcGCQxBzAx5VDqPqHXhQjAqSkdnAYHQCRAkfkAlVDjCI', 'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=data_patent, headers=headers)\n",
    "if response.status_code != requests.codes.ok:\n",
    "    print(response.status_code)\n",
    "\n",
    "else:\n",
    "    print(response.json)\n",
    "data_dict.append(response.json())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'total': 1,\n",
       "  'data': [{'lens_id': '104-626-853-296-452',\n",
       "    'jurisdiction': 'WO',\n",
       "    'date_published': '2021-12-16',\n",
       "    'doc_key': 'WO_2021250546_A1_20211216',\n",
       "    'biblio': {'invention_title': [{'text': 'MICRORNAS AS PREDICTORS OF RESPONSE TO ANTI-IGE THERAPIES IN CHRONIC SPONTANEOUS URTICARIA',\n",
       "       'lang': 'en'},\n",
       "      {'text': \"MICRO-ARN EN TANT QUE PRÉDICTEURS DE RÉPONSE À DES THÉRAPIES ANTI-IGE DANS L'URTICAIRE CHRONIQUE SPONTANÉE\",\n",
       "       'lang': 'fr'}],\n",
       "     'parties': {'applicants': [{'residence': 'US',\n",
       "        'extracted_name': {'value': 'UNIV WASHINGTON'}}],\n",
       "      'inventors': [{'residence': 'US',\n",
       "        'sequence': 1,\n",
       "        'extracted_name': {'value': 'HENDERSON WILLIAM R'}},\n",
       "       {'residence': 'US',\n",
       "        'sequence': 2,\n",
       "        'extracted_name': {'value': 'MACDONALD JAMES W'}},\n",
       "       {'residence': 'US',\n",
       "        'sequence': 3,\n",
       "        'extracted_name': {'value': 'BAMMLER THEODOR K'}},\n",
       "       {'residence': 'US',\n",
       "        'sequence': 4,\n",
       "        'extracted_name': {'value': 'AL-SHAIKHLY TAHA'}}],\n",
       "      'agents': [{'extracted_name': {'value': 'FITZGERALD, Mark J. et al.'},\n",
       "        'extracted_address': 'Nixon Peabody LLP, 53 State Street, Exchange Place, Boston, Massachusetts 02109, 02109',\n",
       "        'extracted_country': 'US'}]},\n",
       "     'cited_by': {}},\n",
       "    'publication_type': 'PATENT_APPLICATION'}],\n",
       "  'results': 1}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = []\n",
    "url = 'https://api.lens.org/patent/search'\n",
    "\n",
    "# include fields\n",
    "include = '''[\"doc_key\",\"biblio.parties\", \"biblio.invention_title\", \"biblio.references_cited\", \"biblio.cited_by\", \"jurisdiction\", \"date_published\", \"publication_type\"]'''\n",
    "\n",
    "\n",
    "# request body with scroll time of 1 minute\n",
    "request_body = '''{\n",
    "  \"query\": {\n",
    "      \"terms\":  {\n",
    "          \"lens_id\": %s\n",
    "      }\n",
    "  },\n",
    "  \"include\": %s\n",
    "  , \"size\": 100\n",
    "  , \"scroll\": \"1m\"\n",
    "}''' % (data, include)\n",
    "headers = {'Authorization': 'IjpxxGZZQcGCQxBzAx5VDqPqHXhQjAqSkdnAYHQCRAkfkAlVDjCI', 'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "# Recursive function to scroll through paginated results\n",
    "def scroll(scroll_id):\n",
    "  # Change the request_body to prepare for next scroll api call\n",
    "  # Make sure to append the include fields to make faster response\n",
    "    if scroll_id is not None:        \n",
    "        global request_body\n",
    "        request_body = '''{\"scroll_id\": \"%s\", \"include\": %s}''' % (scroll_id, include)\n",
    "        \n",
    "    # make api request\n",
    "    response = requests.post(url, data=request_body, headers=headers) \n",
    "    \n",
    "    # If rate-limited, wait for n seconds and proceed the same scroll id\n",
    "    # Since scroll time is 1 minutes, it will give sufficient time to wait and proceed\n",
    "    if response.status_code == requests.codes.too_many_requests:\n",
    "        time.sleep(8)\n",
    "        scroll(scroll_id)\n",
    "    # If the response is not ok here, better to stop here and debug it\n",
    "    \n",
    "    elif response.status_code != requests.codes.ok:\n",
    "        print(response.json())\n",
    "        print('Äahh')\n",
    "    # If the response is ok, do something with the response, take the new scroll id and iterate\n",
    "    \n",
    "    # leave the function if it returns empty (because then everything is downloaded already)\n",
    "    json = response.json()\n",
    "    if json['data'] == []:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        json = response.json()\n",
    "        scroll_id = json['scroll_id'] # Extract the new scroll id from response\n",
    "        #print(json['data']) #DO something with your data\n",
    "        data_dict.append(response.json())\n",
    "        \n",
    "        scroll(scroll_id)\n",
    "        \n",
    "# start recursive scrolling\n",
    "scroll(scroll_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001-535-333-300-333'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the entries\n",
    "data_dict[1]['data'][1]['lens_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177-970-513-795-707\n",
      "168-177-908-076-053\n",
      "152-391-069-052-806\n",
      "023-825-995-431-07X\n",
      "081-101-592-084-931\n",
      "010-439-583-636-995\n",
      "020-946-286-258-253\n",
      "046-826-735-829-232\n",
      "002-541-007-620-216\n",
      "013-896-450-318-443\n",
      "059-578-735-298-090\n",
      "105-178-793-365-769\n",
      "010-250-529-230-290\n",
      "101-329-421-538-77X\n",
      "190-797-046-811-919\n",
      "190-240-428-371-869\n",
      "069-605-151-180-041\n",
      "107-119-408-136-453\n",
      "019-169-055-130-421\n",
      "161-876-210-019-928\n",
      "013-408-491-874-758\n",
      "045-405-085-214-807\n"
     ]
    }
   ],
   "source": [
    "#compare list of stored ids with pervious list \n",
    "d = []\n",
    "for n in data_dict: \n",
    "    #d = d+len(n['data'])\n",
    "    for i in n['data']:\n",
    "        d.append(i['lens_id'])\n",
    "        \n",
    "for i in list(set(df_patent['patentID'])):\n",
    "    if i not in d: \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason lens uses other IDs for these... could be a good question to ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it\n",
    "with open(r'..\\Raw data\\full-data_patent-lens.json', 'w') as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Patents\n",
    "\n",
    "### 3.1. Download and export the raw patent data\n",
    "\n",
    "df_patent = pd.read_csv('../Network data/pub_patent_edges.csv', dtype = 'string')\n",
    "\n",
    "# make list of patentIDs (got to include the double quotes); delete duplicates\n",
    "data = json.dumps(list(set(df_patent['patentID'])))\n",
    "\n",
    "len(list(set(df_patent['patentID'])))\n",
    "\n",
    "## for testing the scraper on one patent\n",
    "# I am not sure about the legal status of these patents (are they still mantained?)\n",
    "\n",
    "data_dict = []\n",
    "start_time = time.time()\n",
    "\n",
    "url = 'https://api.lens.org/patent/search'\n",
    "data_patent = '''{\n",
    "              \"query\": {\n",
    "                  \"terms\":  {\n",
    "                      \"lens_id\": [\"013-408-491-874-758\"]\n",
    "                              }\n",
    "                          },\n",
    "              \"include\": [\"doc_key\",\"biblio.parties\", \"biblio.invention_title\", \"biblio.references_cited\", \"biblio.cited_by\", \"jurisdiction\", \"date_published\", \"publication_type\"]\n",
    "            \n",
    "            }''' \n",
    "headers = {'Authorization': 'IjpxxGZZQcGCQxBzAx5VDqPqHXhQjAqSkdnAYHQCRAkfkAlVDjCI', 'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=data_patent, headers=headers)\n",
    "if response.status_code != requests.codes.ok:\n",
    "    print(response.status_code)\n",
    "\n",
    "else:\n",
    "    print(response.json)\n",
    "data_dict.append(response.json())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "data_dict\n",
    "\n",
    "data_dict = []\n",
    "url = 'https://api.lens.org/patent/search'\n",
    "\n",
    "# include fields\n",
    "include = '''[\"doc_key\",\"biblio.parties\", \"biblio.invention_title\", \"biblio.references_cited\", \"biblio.cited_by\", \"jurisdiction\", \"date_published\", \"publication_type\"]'''\n",
    "\n",
    "\n",
    "# request body with scroll time of 1 minute\n",
    "request_body = '''{\n",
    "  \"query\": {\n",
    "      \"terms\":  {\n",
    "          \"lens_id\": %s\n",
    "      }\n",
    "  },\n",
    "  \"include\": %s\n",
    "  , \"size\": 100\n",
    "  , \"scroll\": \"1m\"\n",
    "}''' % (data, include)\n",
    "headers = {'Authorization': 'IjpxxGZZQcGCQxBzAx5VDqPqHXhQjAqSkdnAYHQCRAkfkAlVDjCI', 'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "# Recursive function to scroll through paginated results\n",
    "def scroll(scroll_id):\n",
    "  # Change the request_body to prepare for next scroll api call\n",
    "  # Make sure to append the include fields to make faster response\n",
    "    if scroll_id is not None:        \n",
    "        global request_body\n",
    "        request_body = '''{\"scroll_id\": \"%s\", \"include\": %s}''' % (scroll_id, include)\n",
    "        \n",
    "    # make api request\n",
    "    response = requests.post(url, data=request_body, headers=headers) \n",
    "    \n",
    "    # If rate-limited, wait for n seconds and proceed the same scroll id\n",
    "    # Since scroll time is 1 minutes, it will give sufficient time to wait and proceed\n",
    "    if response.status_code == requests.codes.too_many_requests:\n",
    "        time.sleep(8)\n",
    "        scroll(scroll_id)\n",
    "    # If the response is not ok here, better to stop here and debug it\n",
    "    \n",
    "    elif response.status_code != requests.codes.ok:\n",
    "        print(response.json())\n",
    "        print('Äahh')\n",
    "    # If the response is ok, do something with the response, take the new scroll id and iterate\n",
    "    \n",
    "    # leave the function if it returns empty (because then everything is downloaded already)\n",
    "    json = response.json()\n",
    "    if json['data'] == []:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        json = response.json()\n",
    "        scroll_id = json['scroll_id'] # Extract the new scroll id from response\n",
    "        #print(json['data']) #DO something with your data\n",
    "        data_dict.append(response.json())\n",
    "        \n",
    "        scroll(scroll_id)\n",
    "        \n",
    "# start recursive scrolling\n",
    "scroll(scroll_id=None)\n",
    "\n",
    "# check out the entries\n",
    "data_dict[1]['data'][1]['lens_id']\n",
    "\n",
    "#compare list of stored ids with pervious list \n",
    "d = []\n",
    "for n in data_dict: \n",
    "    #d = d+len(n['data'])\n",
    "    for i in n['data']:\n",
    "        d.append(i['lens_id'])\n",
    "        \n",
    "for i in list(set(df_patent['patentID'])):\n",
    "    if i not in d: \n",
    "        print(i)\n",
    "\n",
    "# for some reason lens uses other IDs for these... could be a good question to ask\n",
    "\n",
    "len(d)\n",
    "\n",
    "# save it\n",
    "with open(r'..\\Raw data\\full-data_patent-lens.json', 'w') as f:\n",
    "    json.dump(data_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
