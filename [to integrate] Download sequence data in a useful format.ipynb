{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add publication date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "from time import sleep\n",
    "\n",
    "import collections\n",
    "import pandas as pd \n",
    "import re\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the species names (5 most patented wihout hydrothermal vents + 5 most patented from hydrothermal vents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../aar5237_datafiles1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1 = df[df['Included in this study? '] == 'YES'].sort_values(by = 'Number of patented sequences', ascending= False)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6_30 = df[df['Included in this study? '] == 'YES'].sort_values(by = 'Number of patented sequences', ascending= False)[5:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salmo salar', 'synechococcus sp.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(top6_30['Organism'])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to identify record 1 to 100000\n",
      "['CAJNNT000000000.2', 'ON545969.1', 'ON545966.1', 'ON545936.1', 'ON545930.1', 'OW725360.1', 'OW725361.1', 'OW725362.1', 'OW725363.1', 'OW725364.1']\n",
      "Going to identify record 100001 to 200000\n",
      "['ET326618.1', 'ET326617.1', 'ET327384.1', 'ET327383.1', 'ET327382.1', 'ET327381.1', 'ET327380.1', 'ET327379.1', 'ET327378.1', 'ET327377.1']\n",
      "Going to identify record 200001 to 300000\n",
      "['ET227097.1', 'ET227096.1', 'ET227095.1', 'ET227094.1', 'ET227093.1', 'ET227092.1', 'ET227091.1', 'ET227090.1', 'ET227089.1', 'ET225864.1']\n",
      "Going to identify record 300001 to 400000\n",
      "['EG921474.1', 'EG921473.1', 'EG921472.1', 'EG921471.1', 'EG921470.1', 'EG921469.1', 'EG921468.1', 'EG921467.1', 'EG921466.1', 'EG921465.1']\n",
      "Going to identify record 400001 to 500000\n",
      "['EG821474.1', 'EG821473.1', 'EG821472.1', 'EG821471.1', 'EG821470.1', 'EG821469.1', 'EG821468.1', 'EG821467.1', 'EG821466.1', 'EG821465.1']\n",
      "Going to identify record 500001 to 600000\n",
      "['DY710895.1', 'DY710894.1', 'DY710893.1', 'DY710892.1', 'DY710891.1', 'DY710890.1', 'DY710889.1', 'DY710888.1', 'DY710887.1', 'DY710886.1']\n",
      "Going to identify record 600001 to 700000\n",
      "['CX352818.1', 'CX352817.1', 'CX352816.1', 'CX352815.1', 'CX352814.1', 'CX352813.1', 'CX352812.1', 'CX352811.1', 'CX352810.1', 'CX352809.1']\n",
      "Going to identify record 700001 to 800000\n",
      "['BG935197.1', 'BG935196.1', 'BG935195.1', 'BG935194.1', 'BG935193.1', 'BG935192.1', 'BG935191.1', 'BG935190.1', 'BG935189.1', 'BG935188.1']\n",
      "Going to identify record 800001 to 900000\n",
      "['GBHE01005195.1', 'GBHE01005196.1', 'GBHE01005197.1', 'GBHE01005198.1', 'GBHE01005199.1', 'GBHE01005200.1', 'GBHE01005201.1', 'GBHE01005202.1', 'GBHE01005203.1', 'GBHE01005204.1']\n",
      "Going to identify record 900001 to 1000000\n",
      "['JT831408.1', 'JT831407.1', 'JT831406.1', 'JT831405.1', 'JT831404.1', 'JT831403.1', 'JT831402.1', 'JT831401.1', 'JT831400.1', 'JT831399.1']\n",
      "Going to identify record 1000001 to 1100000\n",
      "[]\n",
      "Going to identify record 1 to 100000\n",
      "['ON398846.1', 'BPAC00000000.1', 'BPAC01000001.1', 'BPAC01000002.1', 'BPAC01000003.1', 'BPAC01000004.1', 'BOXD00000000.1', 'BOXD01000001.1', 'BOXD01000002.1', 'BOXD01000003.1']\n",
      "Going to identify record 100001 to 200000\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# this is half finished --> need to create a dictionary of lists in the loop for each organism --> then iterate through this further down \n",
    "batch_size = 100000\n",
    "#count = 500000 # how am I supposed to know this in advance? --> add a function that it should stop once the returned list is empty\n",
    "start = 0\n",
    "\n",
    "AN_dict_perorg = {}\n",
    "empty_list = []\n",
    "\n",
    "for organism in list(top6_30['Organism'])[:2]:    # only 2 for now\n",
    "        ANs = []\n",
    "        print(\"Going to identify record %i to %i\" % (start + 1, start + batch_size))\n",
    "        Entrez.email = 'paul.dunshirn@univie.ac.at'\n",
    "        Entrez.api_key = \"a8c851f7d98c18d6c5ca6d26162e9157e808\"\n",
    "        handle = Entrez.esearch(db=\"nuccore\", term=str(organism)+\"[Primary Organism] AND ddbj_embl_genbank[filter]\", idtype=\"acc\", retstart=start,retmax=batch_size)\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        print(record['IdList'][:10])\n",
    "        ANs.extend(record['IdList'])\n",
    "        \n",
    "        while record['IdList'] != empty_list: \n",
    "            # need to repeat this as long as no empty list is returned (but have to run it once first so that the list is filled with something )\n",
    "            sleep(1)\n",
    "            start = start + batch_size\n",
    "            print(\"Going to identify record %i to %i\" % (start + 1, start + batch_size))\n",
    "            Entrez.email = 'paul.dunshirn@univie.ac.at'\n",
    "            Entrez.api_key = \"a8c851f7d98c18d6c5ca6d26162e9157e808\"\n",
    "            handle = Entrez.esearch(db=\"nuccore\", term=str(organism)+\"[Primary Organism] AND ddbj_embl_genbank[filter]\", idtype=\"acc\", retstart=start,retmax=batch_size)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            print(record['IdList'][:10])\n",
    "            ANs.extend(record['IdList'])\n",
    "        \n",
    "        AN_dict_perorg[str(organism)] = ANs\n",
    "        start = 0\n",
    "            \n",
    "        \n",
    "        #break # take this away if I want all the organisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['salmo salar', 'synechococcus sp.'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AN_dict_perorg.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a csv with AN, origin, pubmedID, authors, publication title, publication date, ... anything else?\n",
    "--> so I will just append multiple lists with these values\n",
    "--> but should also have empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932323\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "for k, v in AN_dict_perorg.items():\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAJNNT000000000.2', 'ON545969.1', 'ON545966.1', 'ON545936.1', 'ON545930.1', 'OW725360.1', 'OW725361.1', 'OW725362.1', 'OW725363.1', 'OW725364.1']\n",
      "['ON398846.1', 'BPAC00000000.1', 'BPAC01000001.1', 'BPAC01000002.1', 'BPAC01000003.1', 'BPAC01000004.1', 'BOXD00000000.1', 'BOXD01000001.1', 'BOXD01000002.1', 'BOXD01000003.1']\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "batch_size = 10\n",
    "for k, v in AN_dict_perorg.items():\n",
    "    print(v[start:start+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salmo salar\n",
      "Going to download record 1 to 500000\n",
      "errorrrrrrrrrrrrr\n",
      "synechococcus sp.\n",
      "Going to download record 1 to 500000\n",
      "--- 128.33128929138184 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # definitely have to reduce this further \n",
    "batch_size = 500000 # this should go up to 400.000, but 500 is recommended by Entrez to not loose anything (?)\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "error_list = []\n",
    "\n",
    "for k, v in AN_dict_perorg.items():\n",
    "    try:\n",
    "        print(k)\n",
    "        all_pubmed_ids = [] # may want to create one big file that also includes missing values, maybe that would be faster than making to separate lists for pubmedIDs (one with country matches and one without)\n",
    "        all_sequence_ANs = []\n",
    "        all_origins = []\n",
    "        i = 0\n",
    "        for start in range(0, len(v), batch_size):\n",
    "            print(\"Going to download record %i to %i\" % (start + 1, start + batch_size))\n",
    "            Entrez.email = 'paul.dunshirn@univie.ac.at'\n",
    "            Entrez.api_key = \"a8c851f7d98c18d6c5ca6d26162e9157e808\"\n",
    "\n",
    "            handle = Entrez.efetch(db=\"nuccore\", id=v[start:start+batch_size], rettype=\"gb\", retmode=\"text\", idtype=\"acc\") # , retstart=start,retmax=batch_size\n",
    "\n",
    "            for gb_record in SeqIO.parse(handle, 'genbank'):  \n",
    "\n",
    "                country = '' # this is not really needed, only in case I want to show countries and non-countries                \n",
    "                if 'country' in gb_record.features[0].qualifiers:\n",
    "                    country = re.findall(\"^(?:(?!:)(?!def).)*\",str(gb_record.features[0].qualifiers['country'][0]))[0]\n",
    "\n",
    "                all_origins.append(country)\n",
    "\n",
    "                all_sequence_ANs.append(gb_record.name)\n",
    "\n",
    "                medID = '' \n",
    "                if gb_record.annotations['references'][0].pubmed_id: \n",
    "                    medID = gb_record.annotations['references'][0].pubmed_id\n",
    "                all_pubmed_ids.append(medID)\n",
    "            i = i+1\n",
    "            if i > 1: \n",
    "                break\n",
    "\n",
    "        data_dict[k] = {'ANs':all_sequence_ANs, 'pubmed':all_pubmed_ids, 'origins':all_origins}\n",
    "    except:\n",
    "        print('errorrrrrrrrrrrrr')\n",
    "        error_list.append(k)\n",
    "        \n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> then I have an error list for which I can do it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['synechococcus sp.'])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to adjust this still to export as Json\n",
    "dictdata = {'origin': all_origins, 'pubMed': all_pubmed_ids, 'ANs': all_sequence_ANs} # maybe I have to write it as dict instead of dictdata? \n",
    "df = pd.DataFrame(dictdata)\n",
    "df.to_csv('C:/Users/pauld/Documents/GitHub/Genetic sequence flows/Data/Genetic sequence data/Entrezdata.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>pubMed</th>\n",
       "      <th>ANs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>OF274803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>OF274776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>OF274680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>OF273781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>OF273744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LU584487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LU584486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LU584485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LU584484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LU584483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      origin pubMed       ANs\n",
       "0                    OF274803\n",
       "1                    OF274776\n",
       "2                    OF274680\n",
       "3                    OF273781\n",
       "4                    OF273744\n",
       "...      ...    ...       ...\n",
       "49995                LU584487\n",
       "49996                LU584486\n",
       "49997                LU584485\n",
       "49998                LU584484\n",
       "49999                LU584483\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the looking up samples code here to see whether I can find out anything else? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibly add a search in open-access papers here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to download a limited list of sequence data\n",
    "Entrez.email = \"paul.dunshirn@univie.ac.at\"\n",
    "Entrez.api_key = \"a8c851f7d98c18d6c5ca6d26162e9157e808\"\n",
    "\n",
    "all_origins = []\n",
    "all_sequence_ANss = []\n",
    "\n",
    "with Entrez.efetch(db=\"nucleotide\", rettype=\"gb\", retmode=\"text\", id=takifugu_ids[:50], idtype=\"acc\") as handle:\n",
    "    for gb_record in SeqIO.parse(handle, \"gb\"):\n",
    "            country = '' # this is not really needed, only in case I want to show countries and non-countries                \n",
    "            if 'country' in gb_record.features[0].qualifiers:\n",
    "                country = re.findall(\"^(?:(?!:)(?!def).)*\",str(gb_record.features[0].qualifiers['country'][0]))[0]\n",
    "            \n",
    "            all_origins.append(country)\n",
    "            \n",
    "            all_sequence_ANss.append(gb_record.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: ON545966.1\n",
      "Name: ON545966\n",
      "Description: Salmo salar isolate Food_Kit_N_Z cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial\n",
      "Number of features: 3\n",
      "/molecule_type=DNA\n",
      "/topology=linear\n",
      "/data_file_division=VRT\n",
      "/date=24-MAY-2022\n",
      "/accessions=['ON545966']\n",
      "/sequence_version=1\n",
      "/keywords=['']\n",
      "/source=mitochondrion Salmo salar (Atlantic salmon)\n",
      "/organism=Salmo salar\n",
      "/taxonomy=['Eukaryota', 'Metazoa', 'Chordata', 'Craniata', 'Vertebrata', 'Euteleostomi', 'Actinopterygii', 'Neopterygii', 'Teleostei', 'Protacanthopterygii', 'Salmoniformes', 'Salmonidae', 'Salmoninae', 'Salmo']\n",
      "/references=[Reference(title='Direct Submission', ...)]\n",
      "/structured_comment=OrderedDict([('Assembly-Data', OrderedDict([('Sequencing Technology', 'Sanger dideoxy sequencing')]))])\n",
      "Seq('GGCACCCTCTATTTAGTATTTGGTGCCTGAGCCGGAATAGTCGGCACCGCCCTA...CTG')\n"
     ]
    }
   ],
   "source": [
    "# this is to download 1 example of sequence data\n",
    "handle = Entrez.efetch(db=\"nuccore\", id='ON545966.1',rettype=\"gb\", retmode=\"text\") #rettype=\"gb\",\n",
    "Entrez.email = 'paul.dunshirn@univie.ac.at'\n",
    "record = SeqIO.read(handle, \"genbank\")\n",
    "handle.close()\n",
    "print(record.annotations['references'][0].pubmed_id)\n",
    "print(record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
